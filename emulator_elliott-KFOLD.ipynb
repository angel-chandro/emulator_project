{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b518b04cbfe0"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb291b62b1aa"
   },
   "source": [
    "# Training + evaluation + testing with the built-in methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d4ac441b1fc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T20:10:26.076907Z",
     "iopub.status.busy": "2021-11-12T20:10:26.076304Z",
     "iopub.status.idle": "2021-11-12T20:10:27.990203Z",
     "shell.execute_reply": "2021-11-12T20:10:27.990591Z"
    },
    "id": "0472bf67b2bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 21:22:09.568731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 21:22:09.568750: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (974861548.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    plots = ['KLF_z0','rLF_z0','earlysize_z0','latesize_z0','HIMF_z0','earlyfraction_z0','TF_z0',&\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_style\n",
    "plt.style.use(mpl_style.style1)\n",
    "\n",
    "def cut_plot(lbins,ubins,data):\n",
    "    # carry out the removal of bins on the plots\n",
    "    # lbins: nº of bins to remove on the left side\n",
    "    # ubins: nº of bins to remove on the right side\n",
    "    bins = data[lbins:-ubins,0]\n",
    "    output = data[lbins:-ubins,1:]\n",
    "    return bins,output\n",
    "\n",
    "nparam = 10\n",
    "nmodels = 1000\n",
    "sim = 'UNIT100' # simulation used\n",
    "# array with the name of the plots we want to use\n",
    "plots = ['KLF_z0','rLF_z0','early-t_z0','late-t_z0','HIMF_z0','early-f_z0','TF_z0',\n",
    "         'bulge-BH_z0','Zstars_z0','KLF_z1.1']\n",
    "xlabel = []\n",
    "ylabel = []\n",
    "xlim = []\n",
    "ylim = []\n",
    "# weight for each plot in the emulator training\n",
    "weight = [2,2,1,1,3,1,1,1,1,2,1]\n",
    "# cuts\n",
    "lcut = []\n",
    "ucut = []\n",
    "\n",
    "def check_cut(ind,nmodels,bins,output,xlab,ylab):\n",
    "    # check the cuts that has just been made in a plot\n",
    "    # to see if the ranges are correct\n",
    "    count = 0\n",
    "    fig = plt.figure(ind)\n",
    "    for i in range(nmodels):\n",
    "        count += np.shape(np.where(output[:,i]==0))[1]\n",
    "        plt.plot(bins[:,0],output[:,i])\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.show()\n",
    "    return count\n",
    "\n",
    "# loading data\n",
    "# DATA MUST BE ALREADY SHUFFLE (no K-FOLD case)\n",
    "len_b = np.array([])\n",
    "bins = np.array([])\n",
    "output = np.array([])\n",
    "for i in range(len(plots)):\n",
    "    # load data from file into bins and output\n",
    "    # len_b to see the length of each plot\n",
    "    file = plots[i]+sim+'.dat'\n",
    "    data = np.loadtxt(file)\n",
    "    bins0,output0 = cut_plot(lbins,ubins,data)\n",
    "    count = check_cut(0,nmodels,bins0,output0,)\n",
    "    print(count)\n",
    "    len_b = np.concatenate([len_b,len(bins0)])\n",
    "    bins = np.concatenate([bins,bins0])\n",
    "    output = np.concatenate([output,output0])\n",
    "\n",
    "plt.close('all')\n",
    "nbins = len(bins)\n",
    "        \n",
    "# input free parameters (Latin Hypercube)\n",
    "# DATA ALREADY SHUFFLE IN THE SAME WAY AS OUTPUT (no K-FOLD case)\n",
    "input_p = np.loadtxt('input_shuffle.dat')\n",
    "\n",
    "    \n",
    "# divide training (80%), evaluation (10%), test (10%)\n",
    "n_train = 0.8*nmodels\n",
    "n_eval = 0.1*nmodels\n",
    "n_test = 0.1*nmodels\n",
    "\n",
    "output = np.transpose(output)\n",
    "output_test = output[:n_test]\n",
    "input_test = input_p[:n_test]\n",
    "output_training = output[n_test:]\n",
    "input_training = input_p[n_test:]\n",
    "# shuffling data train and evaluation\n",
    "#np.random.shuffle(data_train)\n",
    "input_train = input_training[:n_train]\n",
    "output_train = output_training[:n_train]\n",
    "input_eval = input_training[n_train:]\n",
    "output_eval = output_training[n_train:]\n",
    "\n",
    "(x_train, y_train) = (input_train, output_train)\n",
    "(x_eval, y_eval) = (input_eval, output_eval)\n",
    "(x_test, y_test) = (input_test, output_test)\n",
    "(x_data, y_data) = (input_p, output)\n",
    "\n",
    "# Latin Hypercube distribution of free parameters\n",
    "# over parameter space\n",
    "# 10 free parameters\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "def plot_LH(ind,i1,i2,xlab,ylab,xlim,ylim):\n",
    "    # plot the LH points\n",
    "    fig = plt.figure(ind,figsize=(9.8,9.8))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(x_train[:,i1],x_train[:,i2],'.b',markersize=15,label='Training')\n",
    "    ax.plot(x_eval[:,i1],x_eval[:,i2],'.r',markersize=15,label='Evaluation')\n",
    "    ax.plot(x_test[:,i1],x_test[:,i2],'.g',markersize=15,label='Test')\n",
    "    ax.set_xlabel(xlab)\n",
    "    ax.set_ylabel(ylab)\n",
    "    ax.set_xlim(xlim[0],xlim[1])\n",
    "    ax.set_ylim(ylim[0],ylim[1])\n",
    "    # Shrink current axis by 20%\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.set_box_aspect(1)\n",
    "    # Put a legend to the right of the current axis\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "for i in range(nparam):\n",
    "    for j in range(nparam):\n",
    "        plot_LH(i+j,i,j,)\n",
    "        \n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# free parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m a_cool \u001b[38;5;241m=\u001b[39m \u001b[43minput_data\u001b[49m[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m v_SN \u001b[38;5;241m=\u001b[39m input_data[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m F_stab \u001b[38;5;241m=\u001b[39m input_data[:,\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "def check_training(ind,n_train,bins,output,xlab,ylab):\n",
    "    # check the behaviour of the training model\n",
    "    # and if they span the parameter space properly\n",
    "    fig = plt.figure(ind)\n",
    "    for i in range(n_train):\n",
    "        plt.plot(bins[:,0],output[:,i])\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "acum = 0\n",
    "for i in range(len(plots)):\n",
    "    \n",
    "    if plots[i]=='KLF_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[0]],output[acum:acum+len_b[0]])\n",
    "        acum += len_b[0]\n",
    "    elif plots[i]=='rLF_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[1]],output[acum:acum+len_b[1]])\n",
    "        acum += len_b[1]\n",
    "    elif plots[i]=='early-t_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[2]],output[acum:acum+len_b[2]])\n",
    "        acum += len_b[2]\n",
    "    elif plots[i]=='late-t_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[3]],output[acum:acum+len_b[3]])\n",
    "        acum += len_b[3]\n",
    "    elif plots[i]=='HIMF_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[4]],output[acum:acum+len_b[4]])\n",
    "        acum += len_b[4]\n",
    "    elif plots[i]=='early-f_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[5]],output[acum:acum+len_b[5]])\n",
    "        acum += len_b[5]\n",
    "    elif plots[i]=='TF_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[6]],output[acum:acum+len_b[6]])\n",
    "        acum += len_b[6]\n",
    "    elif plots[i]=='bulge-BH_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[7]],output[acum:acum+len_b[7]])\n",
    "        acum += len_b[7]\n",
    "    elif plots[i]=='Zstars_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[8]],output[acum:acum+len_b[8]])\n",
    "        acum += len_b[8]\n",
    "    elif plots[i]=='KLF_z1.1':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[9]],output[acum:acum+len_b[9]])\n",
    "        acum += len_b[9]\n",
    "    elif plots[i]=='mgasf_z0':\n",
    "        check_training(0,n_train,bins[acum:acum+len_b[10]],output[acum:acum+len_b[10]])\n",
    "        acum += len_b[10]\n",
    "        \n",
    "plt.close('all')\n",
    "\n",
    "# SOLVE PROBLEM IF KLF_z=1.1 IS NOT USED AND MASS GAS FRACTION IS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6d5724a90ab"
   },
   "source": [
    "Here's what the typical end-to-end workflow looks like, consisting of:\n",
    "\n",
    "- Training\n",
    "- Validation on a holdout set generated from the original training data\n",
    "- Evaluation on the test data\n",
    "\n",
    "We'll use MNIST data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T20:10:29.664702Z",
     "iopub.status.busy": "2021-11-12T20:10:29.664118Z",
     "iopub.status.idle": "2021-11-12T20:10:30.037955Z",
     "shell.execute_reply": "2021-11-12T20:10:30.037400Z"
    },
    "id": "8b55b3903edb"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train_f = x_train.astype(\"float32\")\n",
    "x_eval_f = x_eval.astype(\"float32\")\n",
    "x_test_f = x_test.astype(\"float32\")\n",
    "y_train_f = y_train.astype(\"float32\")\n",
    "y_eval_f = y_eval.astype(\"float32\")\n",
    "y_test_f = y_test.astype(\"float32\")\n",
    "\n",
    "y_data_f = y_data.astype(\"float32\")\n",
    "y_data_f = y_data.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77a84eb1985b"
   },
   "source": [
    "We specify the training configuration (optimizer, loss, metrics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T20:10:30.044502Z",
     "iopub.status.busy": "2021-11-12T20:10:30.043891Z",
     "iopub.status.idle": "2021-11-12T20:10:30.055119Z",
     "shell.execute_reply": "2021-11-12T20:10:30.054690Z"
    },
    "id": "26a7f1819796"
   },
   "outputs": [],
   "source": [
    "(x_data_train_f, y_data_train_f) = (input_training, output_training)\n",
    "(x_test_f, y_test_f) = (input_test, output_test)\n",
    "\n",
    "\n",
    "x_data_train_f = x_data_train_f.astype(\"float32\")\n",
    "x_test_f = x_test_f.astype(\"float32\")\n",
    "y_data_train_f = y_data_train_f.astype(\"float32\")\n",
    "y_test_f = y_test_f.astype(\"float32\")\n",
    "\n",
    "# define KFOLD\n",
    "# here the test data is the same, and we vary the training and the validation set\n",
    "nsplit = 9 # split into 9 (100 validation of 900=800+100)\n",
    "kf = KFold(n_splits=nsplit,shuffle=False)\n",
    "save_dir = 'saved_models_kfold'\n",
    "fold_var = 1\n",
    "nepoch = 500 # to impose the condition when to stop the training\n",
    "\n",
    "def get_model():\n",
    "    # define the emulator configuration\n",
    "    inputs = keras.Input(shape=(nparam,), name=\"digits\")\n",
    "    x = layers.Dense(512, activation=tf.keras.activations.sigmoid, name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(512, activation=tf.keras.activations.sigmoid, name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(nbins, activation=\"linear\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(amsgrad=True,name='Adam_ams'),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss='MAE',\n",
    "    # List of metrics to monitor\n",
    "    metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "for train_idx, val_idx in list(kf.split(x_data_train_f,y_data_train_f)):\n",
    "    \n",
    "    x_train_f = x_data_train_f[train_idx]\n",
    "    y_train_f = y_data_train_f[train_idx]\n",
    "    x_eval_f = x_data_train_f[val_idx]\n",
    "    y_eval_f = y_data_train_f[val_idx]\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # only update the emulator when it improves its performance over the validation data\n",
    "    # stop if nepoch have passed without improvement\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='loss', patience=nepoch, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    # TRAINING\n",
    "    print(\"Fit model on training data\")\n",
    "    history = model.fit(\n",
    "        x_train_f,\n",
    "        y_train_f,\n",
    "        batch_size=1,\n",
    "        epochs=10000,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(x_eval_f, y_eval_f),\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "    loss.append(history.history['loss'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    loss_p = np.array(loss)\n",
    "    val_loss_p = np.array(val_loss)\n",
    "    loss_p = loss_p.flatten()\n",
    "    val_loss_p = val_loss_p.flatten()\n",
    "    \n",
    "    # freezing\n",
    "    # compile and retrain with a low learning rate\n",
    "    low_lr = 1e-5\n",
    "    model.compile(loss='MAE',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(learning_rate=low_lr), \n",
    "                  metrics=['accuracy']\n",
    "    )\n",
    "    history = model.fit(\n",
    "        x_train_f,\n",
    "        y_train_f,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(x_eval_f, y_eval_f),\n",
    "    )\n",
    "    \n",
    "    loss_f = np.array(history.history['loss'])\n",
    "    val_loss_f = np.array(history.history['val_loss'])\n",
    "\n",
    "    loss_t = np.concatenate((loss_p,loss_f))\n",
    "    val_loss_t = np.concatenate((val_loss_p,val_loss_f))\n",
    "    loss_t = loss_t.flatten()\n",
    "    val_loss_t = val_loss_t.flatten()\n",
    "\n",
    "    # plot training curves with freezing\n",
    "    plt.figure(figsize=(9.8,7.2))\n",
    "    plt.plot(np.linspace(1,len(loss_t),len(loss_t)),loss_t,'-b',label='Training data')\n",
    "    plt.plot(np.linspace(1,len(loss_t),len(loss_t)),val_loss_t,'-r',label='Validation data')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlim(0,len(loss_t))\n",
    "    #plt.ylim(0,0.2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    outfil = save_dir+'performance_em_'+str(fold_var)+'.dat'\n",
    "    tofile = zip(loss_t,val_loss_t)\n",
    "    with open(outfil, 'w') as outf: # written mode (not appended)\n",
    "        outf.write('# MAE training, MAE validation \\n')\n",
    "        np.savetxt(outf,list(tofile))#,fmt=('%.5f'))\n",
    "        outf.closed \n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    # the same test data all the time\n",
    "    print(x_test_f.shape)\n",
    "    print(y_test_f.shape)\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(x_test_f, y_test_f, batch_size=1)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for samples\")\n",
    "    if fold_var == 1:\n",
    "        predictions1 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions1.shape)\n",
    "    elif fold_var == 2:\n",
    "        predictions2 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions2.shape)\n",
    "    elif fold_var == 3:\n",
    "        predictions3 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions3.shape)\n",
    "    elif fold_var == 4:\n",
    "        predictions4 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions4.shape)\n",
    "    elif fold_var == 5:\n",
    "        predictions5 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions5.shape)\n",
    "    elif fold_var == 6:\n",
    "        predictions6 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions6.shape)\n",
    "    elif fold_var == 7:\n",
    "        predictions7 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions7.shape)\n",
    "    elif fold_var == 8:\n",
    "        predictions8 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions8.shape)\n",
    "    elif fold_var == 9:\n",
    "        predictions9 = model.predict(x_test_f)\n",
    "        print(\"predictions shape:\", predictions9.shape)\n",
    "    #print(y_test)\n",
    "\n",
    "    # check that the test MAE is simply the mean\n",
    "    mae_a = []\n",
    "    maeObject = keras.losses.MeanAbsoluteError()\n",
    "    for i in range(n_test):\n",
    "        maeTensor = maeObject(y_test_f[i,:], predictions[i])\n",
    "        mae = maeTensor.numpy()\n",
    "        print(mae)\n",
    "        mae_a.append(mae)\n",
    "    mae_a = np.array(mae_a)    \n",
    "\n",
    "    print(model.metrics_names)\n",
    "\n",
    "    # save each of the nsplit models\n",
    "    model.save(save_dir+str(fold_var))\n",
    "    \n",
    "    fold_var +=1\n",
    "\n",
    "# generate the final prediction of the test data\n",
    "predictions = (predictions1+predictions2+predictions3+predictions4+predictions5+predictions6+predictions7+predictions8+predictions9)/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(ind,n_train,galf,pred,xlab,ylab,xlim,ylim,file,comm):\n",
    "    # compare the predictions of the test and the real values visibly\n",
    "    # the closer to the y=x curve, the more accurate the emulator performance is\n",
    "    fig = plt.figure(ind,figsize=(9.6,7.2))\n",
    "    for i,c in zip(range(n_test),color):\n",
    "        plt.plot(galf[i,:],pred[i,:],'.',c=c,markersize=10)\n",
    "    plt.plot(np.linspace(xlim[0],xlim[1],100),np.linspace(ylim[0],ylim[1],100),'-k')\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.show()\n",
    "    \n",
    "    #galf = galf.flatten()\n",
    "    #pred = pred.flatten()\n",
    "    # ratio\n",
    "    #tofile = zip(galf,pred/galf)\n",
    "    #with open(file, 'w') as outf: # written mode (not appended)\n",
    "    #    outf.write(comm)\n",
    "    #    np.savetxt(outf,list(tofile))#,fmt=('%.5f'))\n",
    "    #    outf.closed \n",
    "    return\n",
    "\n",
    "acum = 0\n",
    "for i in range(len(plots)):\n",
    "    \n",
    "    if plots[i]=='KLF_z0':\n",
    "        check_test(0,n_train,output_test[acum:acum+len_b[0]],predictions[acum:acum+len_b[0]])\n",
    "        acum += len_b[0]\n",
    "    elif plots[i]=='rLF_z0':\n",
    "        check_test(1,n_train,output_test[acum:acum+len_b[1]],predictions[acum:acum+len_b[1]])\n",
    "        acum += len_b[1]\n",
    "    elif plots[i]=='early-t_z0':\n",
    "        check_test(2,n_train,output_test[acum:acum+len_b[2]],predictions[acum:acum+len_b[2]])\n",
    "        acum += len_b[2]\n",
    "    elif plots[i]=='late-t_z0':\n",
    "        check_test(3,n_train,output_test[acum:acum+len_b[3]],predictions[acum:acum+len_b[3]])\n",
    "        acum += len_b[3]\n",
    "    elif plots[i]=='HIMF_z0':\n",
    "        check_test(4,n_train,output_test[acum:acum+len_b[4]],predictions[acum:acum+len_b[4]])\n",
    "        acum += len_b[4]\n",
    "    elif plots[i]=='early-f_z0':\n",
    "        check_test(5,n_train,output_test[acum:acum+len_b[5]],predictions[acum:acum+len_b[5]])\n",
    "        acum += len_b[5]\n",
    "    elif plots[i]=='TF_z0':\n",
    "        check_test(6,n_train,output_test[acum:acum+len_b[6]],predictions[acum:acum+len_b[6]])\n",
    "        acum += len_b[6]\n",
    "    elif plots[i]=='bulge-BH_z0':\n",
    "        check_test(7,n_train,output_test[acum:acum+len_b[7]],predictions[acum:acum+len_b[7]])\n",
    "        acum += len_b[7]\n",
    "    elif plots[i]=='Zstars_z0':\n",
    "        check_test(8,n_train,output_test[acum:acum+len_b[8]],predictions[acum:acum+len_b[8]])\n",
    "        acum += len_b[8]\n",
    "    elif plots[i]=='KLF_z1.1':\n",
    "        check_test(9,n_train,output_test[acum:acum+len_b[9]],predictions[acum:acum+len_b[9]])\n",
    "        acum += len_b[9]\n",
    "    elif plots[i]=='mgasf_z0':\n",
    "        check_test(10,n_train,output_test[acum:acum+len_b[10]],predictions[acum:acum+len_b[10]])\n",
    "        acum += len_b[10]\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "def check_test2(ind,n_train,n_test,bins,output,galf,pred,xlab,ylab,xlim,ylim):\n",
    "    # compare the training models (real values) with the test models (real values)\n",
    "    # to check if both of them span correctly the parameter space\n",
    "    fig = plt.figure(ind,figsize=(9.6,7.2))\n",
    "    for i in range(n_train):\n",
    "        if i==0:\n",
    "            plt.plot(bins[:,0],output[:,i],c='lightgrey',ls='-',label='Galform training')  \n",
    "        else:\n",
    "            plt.plot(bins[:,0],output[:,i],c='lightgrey',ls='-')  \n",
    "    for i, c in zip(range(n_test), color):\n",
    "        if i==0:\n",
    "            plt.plot(bins[:,0],galf[i,:],'-',c=c,linewidth=2.5,label='Galform test')\n",
    "            plt.plot(bins[:,0],pred[i,:],':',c=c,linewidth=3,label='Emulator',zorder=200)\n",
    "        else:\n",
    "            plt.plot(bins[:,0],galf[i,:],'-',c=c,linewidth=2.5)\n",
    "            plt.plot(bins[:,0],pred[i,:],':',c=c,linewidth=3)    \n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    # Put a legend to the right of the current axis\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "acum = 0\n",
    "for i in range(len(plots)):\n",
    "    \n",
    "    if plots[i]=='KLF_z0':\n",
    "        check_test2(0,n_train,n_test,bins[acum:acum+len_b[0]],output_train[acum:acum+len_b[0]],output_test[acum:acum+len_b[0]],predictions[acum:acum+len_b[0]])\n",
    "        acum += len_b[0]\n",
    "    elif plots[i]=='rLF_z0':\n",
    "        check_test2(1,n_train,n_test,bins[acum:acum+len_b[1]],output_train[acum:acum+len_b[1]],output_test[acum:acum+len_b[1]],predictions[acum:acum+len_b[1]])\n",
    "        acum += len_b[1]\n",
    "    elif plots[i]=='early-t_z0':\n",
    "        check_test2(2,n_train,n_test,bins[acum:acum+len_b[2]],output_train[acum:acum+len_b[2]],output_test[acum:acum+len_b[2]],predictions[acum:acum+len_b[2]])\n",
    "        acum += len_b[2]\n",
    "    elif plots[i]=='late-t_z0':\n",
    "        check_test2(3,n_train,n_test,bins[acum:acum+len_b[3]],output_train[acum:acum+len_b[3]],output_test[acum:acum+len_b[3]],predictions[acum:acum+len_b[3]])\n",
    "        acum += len_b[3]\n",
    "    elif plots[i]=='HIMF_z0':\n",
    "        check_test2(4,n_train,n_test,bins[acum:acum+len_b[4]],output_train[acum:acum+len_b[4]],output_test[acum:acum+len_b[4]],predictions[acum:acum+len_b[4]])\n",
    "        acum += len_b[4]\n",
    "    elif plots[i]=='early-f_z0':\n",
    "        check_test2(5,n_train,n_test,bins[acum:acum+len_b[5]],output_train[acum:acum+len_b[5]],,output_test[acum:acum+len_b[5]],predictions[acum:acum+len_b[5]])\n",
    "        acum += len_b[5]\n",
    "    elif plots[i]=='TF_z0':\n",
    "        check_test2(6,n_train,n_test,bins[acum:acum+len_b[6]],output_train[acum:acum+len_b[6]],output_test[acum:acum+len_b[6]],predictions[acum:acum+len_b[6]])\n",
    "        acum += len_b[6]\n",
    "    elif plots[i]=='bulge-BH_z0':\n",
    "        check_test2(7,n_train,n_test,bins[acum:acum+len_b[7]],output_train[acum:acum+len_b[7]],output_test[acum:acum+len_b[7]],predictions[acum:acum+len_b[7]])\n",
    "        acum += len_b[7]\n",
    "    elif plots[i]=='Zstars_z0':\n",
    "        check_test2(8,n_train,n_test,bins[acum:acum+len_b[8]],output_train[acum:acum+len_b[8]],output_test[acum:acum+len_b[8]],predictions[acum:acum+len_b[8]])\n",
    "        acum += len_b[8]\n",
    "    elif plots[i]=='KLF_z1.1':\n",
    "        check_test2(9,n_train,n_test,bins[acum:acum+len_b[9]],output_train[acum:acum+len_b[9]],output_test[acum:acum+len_b[9]],predictions[acum:acum+len_b[9]])\n",
    "        acum += len_b[9]\n",
    "    elif plots[i]=='mgasf_z0':\n",
    "        check_test2(10,n_train,n_test,bins[acum:acum+len_b[10]],output_train[acum:acum+len_b[10]],output_test[acum:acum+len_b[10]],predictions[acum:acum+len_b[10]])\n",
    "        acum += len_b[10]\n",
    "\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_and_evaluate.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
